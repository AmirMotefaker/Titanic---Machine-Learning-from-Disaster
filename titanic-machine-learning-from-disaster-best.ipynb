{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/titanic-machine-learning-from-disaster-best?scriptVersionId=125165410\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\n- The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\n- On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n- While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n- In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm # Statistical computations and models for Python\n\nfrom statsmodels.nonparametric.kde import KDEUnivariate\nfrom statsmodels.nonparametric import smoothers_lowess\n\nfrom pandas import Series, DataFrame\n\nfrom patsy import dmatrices # A Python package for describing statistical models and for building design matrices.\n\nfrom sklearn import datasets, svm\n\n# from KaggleAux import predict as ka # see github.com/agconti/kaggleaux for more details\n# KaggleAux is a collection of statistical tools to aid Data Science competitors in Kaggle Competitions.\n# pip install gnureadline, ipython, matplotlib, mock, nose, numpy, pandas, pyparsing, python-dateutil, pytz, six, wsgiref, scipy, statsmodels, patsy\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Handling","metadata":{}},{"cell_type":"code","source":"# Let's read our data in using pandas:\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show an overview of our data:\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looking at the data frame above:\n\n- First, it lets us know we have 891 observations, or passengers, to analyze here:\n\n    - Int64Index: 891 entries, 0 to 890\n\n- Next it shows us all of the columns in DataFrame. Each column tells us something about each of our observations, like their name, sex, or age. These columns are called features of our dataset. \n\n\n- After each feature it lets us know how many values it contains. While most of our features have complete data on every observation, like the survived feature here:\n\n    - survived    891  non-null values \n\n- some are missing information, like the age feature:\n\n    - age         714  non-null values \n\n\n- These missing values are represented as NaNs.","metadata":{}},{"cell_type":"markdown","source":"#### Take care of missing values:\n\n- The features 'Ticket' and 'Cabin' have many missing values and so can’t add much value to our analysis. To handle this we will drop them from the data frame to preserve the integrity of our dataset.\n\n\n- To do that we'll use this line of code to drop the features entirely:\n\n    - df = df.drop(['ticket','cabin'], axis=1) \n\n\n- While this line of code removes the NaN values from every remaining column/feature:\n\n    - df = df.dropna()\n\n- Now we have a clean and tidy dataset that is ready for analysis. Because .dropna() removes an observation from our data even if it only has 1 NaN in one of the features, it would have removed most of our dataset if we had not dropped the ticket and cabin features first.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['Ticket','Cabin'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove NaN values\ndf = df.dropna() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Take a Look at your data graphically","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Survival, (1 = Survived)\n","metadata":{}},{"cell_type":"code","source":"# specifies the parameters of our graphs\nfig = plt.figure(figsize=(18,6), dpi=1600) \nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\n# lets us plot many diffrent shaped graphs together \nax1 = plt.subplot2grid((2,3),(0,0))\n\n# plots a bar graph of those who surived vs those who did not.               \ndf.Survived.value_counts().plot(kind='bar', alpha=alpha_bar_chart)\n\n# this nicely sets the margins in matplotlib to deal with a recent bug 1.3.1\nax1.set_xlim(-1, 2)\n\n# puts a title on our graph\nplt.title(\"Distribution of Survival, (1 = Survived)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Survival by Age,  (1 = Survived)\n","metadata":{}},{"cell_type":"code","source":"# specifies the parameters of our graphs\nfig = plt.figure(figsize=(18,6), dpi=1600) \nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\nplt.subplot2grid((2,3),(0,1))\nplt.scatter(df.Survived, df.Age, alpha=alpha_scatterplot)\n\n# sets the y axis lable\nplt.ylabel(\"Age\")\n\n# formats the grid line style of our graphs                          \nplt.grid(visible=True, which='major', axis='y')  \nplt.title(\"Survival by Age,  (1 = Survived)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class Distribution\n","metadata":{}},{"cell_type":"code","source":"# specifies the parameters of our graphs\nfig = plt.figure(figsize=(18,6), dpi=1600) \nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\nax3 = plt.subplot2grid((2,3),(0,2))\ndf.Pclass.value_counts().plot(kind=\"barh\", alpha=alpha_bar_chart)\nax3.set_ylim(-1, len(df.Pclass.value_counts()))\nplt.title(\"Class Distribution\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Age Distribution within classes\n","metadata":{}},{"cell_type":"code","source":"# specifies the parameters of our graphs\nfig = plt.figure(figsize=(18,6), dpi=1600) \nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\nplt.subplot2grid((2,3),(1,0), colspan=2)\n\n# plots a kernel density estimate of the subset of the 1st class passangers's age\ndf.Age[df.Pclass == 1].plot(kind='kde')    \ndf.Age[df.Pclass == 2].plot(kind='kde')\ndf.Age[df.Pclass == 3].plot(kind='kde')\n\n# plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\")\n\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Passengers per boarding location\n","metadata":{}},{"cell_type":"code","source":"# specifies the parameters of our graphs\nfig = plt.figure(figsize=(18,6), dpi=1600) \nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\nax5 = plt.subplot2grid((2,3),(1,2))\ndf.Embarked.value_counts().plot(kind='bar', alpha=alpha_bar_chart)\nax5.set_xlim(-1, len(df.Embarked.value_counts()))\n\n# specifies the parameters of our graphs\nplt.title(\"Passengers per boarding location\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Visualization:\n\n- The point of this competition is to predict if an individual will survive based on the features in the data like:\n\n    - Traveling Class (called pclass in the data)\n    - Sex\n    - Age\n    - Fare Price\n\n- Let’s see if we can gain a better understanding of who survived and died.","metadata":{}},{"cell_type":"markdown","source":"### A bar graph of those who survived versus those who died.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nfig, ax = plt.subplots()\ndf.Survived.value_counts().plot(kind='barh', color=\"blue\", alpha=.65)\nax.set_ylim(-1, len(df.Survived.value_counts())) \nplt.title(\"Survival Breakdown (1 = Survived, 0 = Died)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let’s break the previous graph down by gender","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,6))\n\n#create a plot of two subsets, male and female, of the survived variable.\n#After we do that we call value_counts() so it can be easily plotted as a bar graph. \n#'barh' is just a horizontal bar graph\ndf_male = df.Survived[df.Sex == 'male'].value_counts().sort_index()\ndf_female = df.Survived[df.Sex == 'female'].value_counts().sort_index()\n\nax1 = fig.add_subplot(121)\ndf_male.plot(kind='barh',label='Male', alpha=0.55)\ndf_female.plot(kind='barh', color='#FA2379',label='Female', alpha=0.55)\nplt.title(\"Who Survived? with respect to Gender, (raw value counts) \"); plt.legend(loc='best')\nax1.set_ylim(-1, 2) \n\n#adjust graph to display the proportions of survival by gender\nax2 = fig.add_subplot(122)\n(df_male/float(df_male.sum())).plot(kind='barh',label='Male', alpha=0.55)  \n(df_female/float(df_female.sum())).plot(kind='barh', color='#FA2379',label='Female', alpha=0.55)\nplt.title(\"Who Survived proportionally? with respect to Gender\"); plt.legend(loc='best')\n\nax2.set_ylim(-1, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here it’s clear that although more men died and survived in raw value counts, females had a greater survival rate proportionally (~25%), than men (~20%).","metadata":{}},{"cell_type":"markdown","source":"- Can we capture more of the structure by using Pclass? \n- Here we will bucket classes as the lowest class or any of the high classes (classes 1 - 2). 3 is the lowest class. \n- Let’s break it down by Gender and what Class they were traveling in.","metadata":{}},{"cell_type":"markdown","source":"### Who Survived? with respect to Gender and Class","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,4), dpi=1600)\nalpha_level = 0.65\n\n# Building on the previous code, here we create an additional subset within the gender subset \n# we created for the survived variable. That's a lot of subsets. After we do that we call value_counts() \n# so it can be easily plotted as a bar graph. \n# this is repeated for each gender class pair.\nax1=fig.add_subplot(141)\nfemale_highclass = df.Survived[df.Sex == 'female'][df.Pclass != 3].value_counts()\nfemale_highclass.plot(kind='bar', label='female, highclass', color='#FA2479', alpha=alpha_level)\nax1.set_xticklabels([\"Survived\", \"Died\"], rotation=0)\nax1.set_xlim(-1, len(female_highclass))\nplt.title(\"Who Survived? with respect to Gender and Class\"); plt.legend(loc='best')\n\nax2=fig.add_subplot(142, sharey=ax1)\nfemale_lowclass = df.Survived[df.Sex == 'female'][df.Pclass == 3].value_counts()\nfemale_lowclass.plot(kind='bar', label='female, low class', color='pink', alpha=alpha_level)\nax2.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\nax2.set_xlim(-1, len(female_lowclass))\nplt.legend(loc='best')\n\nax3=fig.add_subplot(143, sharey=ax1)\nmale_lowclass = df.Survived[df.Sex == 'male'][df.Pclass == 3].value_counts()\nmale_lowclass.plot(kind='bar', label='male, low class',color='lightblue', alpha=alpha_level)\nax3.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\nax3.set_xlim(-1, len(male_lowclass))\nplt.legend(loc='best')\n\nax4=fig.add_subplot(144, sharey=ax1)\nmale_highclass = df.Survived[df.Sex == 'male'][df.Pclass != 3].value_counts()\nmale_highclass.plot(kind='bar', label='male, highclass', alpha=alpha_level, color='steelblue')\nax4.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\nax4.set_xlim(-1, len(male_highclass))\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now we have a lot more information on who survived and died in the tragedy. \n- With this deeper understanding, we are better equipped to create better more insightful models. \n- This is a typical process in interactive data analysis.\n\n    - First, you start small and understand the most basic relationships and slowly increment the complexity of your analysis as you discover more and more about the data you’re working with. ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,12), dpi=1600)\na = 0.65\n\n# Step 1\nax1 = fig.add_subplot(341)\ndf.Survived.value_counts().plot(kind='bar', color=\"blue\", alpha=a)\nax1.set_xlim(-1, len(df.Survived.value_counts()))\nplt.title(\"Step. 1\")\n\n# Step 2\n# Who Survived? with respect to Gender\nax2 = fig.add_subplot(345)\ndf.Survived[df.Sex == 'male'].value_counts().plot(kind='bar',label='Male')\ndf.Survived[df.Sex == 'female'].value_counts().plot(kind='bar', color='#FA2379',label='Female')\nax2.set_xlim(-1, 2)\nplt.title(\"Step. 2 \\nWho Survived? with respect to Gender.\"); plt.legend(loc='best')\n\n# Who Survied proportionally?\nax3 = fig.add_subplot(346)\n(df.Survived[df.Sex == 'male'].value_counts()/float(df.Sex[df.Sex == 'male'].size)).plot(kind='bar',label='Male')\n(df.Survived[df.Sex == 'female'].value_counts()/float(df.Sex[df.Sex == 'female'].size)).plot(kind='bar', color='#FA2379',label='Female')\nax3.set_xlim(-1,2)\nplt.title(\"Who Survied proportionally?\"); plt.legend(loc='best')\n\n\n# Step 3\n# Who Survived? with respect to Gender and Class\nax4 = fig.add_subplot(349)\nfemale_highclass = df.Survived[df.Sex == 'female'][df.Pclass != 3].value_counts()\nfemale_highclass.plot(kind='bar', label='female highclass', color='#FA2479', alpha=a)\nax4.set_xticklabels([\"Survived\", \"Died\"], rotation=0)\nax4.set_xlim(-1, len(female_highclass))\nplt.title(\"Who Survived? with respect to Gender and Class\"); plt.legend(loc='best')\n\nax5 = fig.add_subplot(3,4,10, sharey=ax1)\nfemale_lowclass = df.Survived[df.Sex == 'female'][df.Pclass == 3].value_counts()\nfemale_lowclass.plot(kind='bar', label='female, low class', color='pink', alpha=a)\nax5.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\nax5.set_xlim(-1, len(female_lowclass))\nplt.legend(loc='best')\n\nax6 = fig.add_subplot(3,4,11, sharey=ax1)\nmale_lowclass = df.Survived[df.Sex == 'male'][df.Pclass == 3].value_counts()\nmale_lowclass.plot(kind='bar', label='male, low class',color='lightblue', alpha=a)\nax6.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\nax6.set_xlim(-1, len(male_lowclass))\nplt.legend(loc='best')\n\nax7 = fig.add_subplot(3,4,12, sharey=ax1)\nmale_highclass = df.Survived[df.Sex == 'male'][df.Pclass != 3].value_counts()\nmale_highclass.plot(kind='bar', label='male highclass', alpha=a, color='steelblue')\nax7.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\nax7.set_xlim(-1, len(male_highclass))\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Supervised Machine Learning","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"- Logistic regression measures the relationship between a categorical dependent variable and one or more independent variables, which are usually (but not necessarily) continuous, by using probability scores as the predicted values of the dependent variable. ","metadata":{}},{"cell_type":"markdown","source":"#### Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"# model formula for Logistic Regression Model\n\n# here the ~ sign is an = sign, and the features of our dataset\n# are written as a formula to predict survived. The C() lets our \n# regression know that those variables are categorical.\n# Ref: http://patsy.readthedocs.org/en/latest/formulas.html\nformula = 'Survived ~ C(Pclass) + C(Sex) + Age + SibSp  + C(Embarked)' \n\n# create a results dictionary to hold our regression results for easy analysis later        \nresults = {} ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a regression friendly dataframe using patsy's dmatrices function\ny,x = dmatrices(formula, data=df, return_type='dataframe')\n\n# instantiate our model\nmodel = sm.Logit(y,x)\n\n# fit our model to the training data\nres = model.fit()\n\n# save the result for outputing predictions later\nresults['Logistic'] = [res, formula]\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Predictions Vs Actual\nplt.figure(figsize=(18,4));\nplt.subplot(121, facecolor=\"#DBDBDB\")\n\n# Generate predictions from our fitted model\nypred = res.predict(x)\nplt.plot(x.index, ypred, 'bo', x.index, y, 'mo', alpha=.25);\nplt.grid(color='white', linestyle='dashed')\nplt.title('Logit predictions, Blue: \\nFitted/predicted values: Red');\n\n# Residuals\nax2 = plt.subplot(122, facecolor=\"#DBDBDB\")\nplt.plot(res.resid_dev, 'r-')\nplt.grid(color='white', linestyle='dashed')\nax2.set_xlim(-1, len(res.resid_dev))\nplt.title('Logit Residuals');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Look at the predictions we generated graphically:","metadata":{}},{"cell_type":"code","source":"# Distribution of our Predictions\nfig = plt.figure(figsize=(18,9), dpi=1600)\na = .2\n\nfig.add_subplot(221, facecolor=\"#DBDBDB\")\nkde_res = KDEUnivariate(res.predict())\nkde_res.fit()\nplt.plot(kde_res.support,kde_res.density)\nplt.fill_between(kde_res.support,kde_res.density, alpha=a)\nplt.title(\"Distribution of our Predictions\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Change of Survival Probability by Gender (1 = Male)\nfig = plt.figure(figsize=(18,9), dpi=1600)\na = .2\n\nfig.add_subplot(222, facecolor=\"#DBDBDB\")\nplt.scatter(res.predict(),x['C(Sex)[T.male]'] , alpha=a)\nplt.grid(visible=True, which='major', axis='x')\nplt.xlabel(\"Predicted chance of survival\")\nplt.ylabel(\"Gender Bool\")\nplt.title(\"The Change of Survival Probability by Gender (1 = Male)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Change of Survival Probability by Lower Class (1 = 3rd Class)\nfig = plt.figure(figsize=(18,9), dpi=1600)\na = .2\n\nfig.add_subplot(223, facecolor=\"#DBDBDB\")\nplt.scatter(res.predict(),x['C(Pclass)[T.3]'] , alpha=a)\nplt.xlabel(\"Predicted chance of survival\")\nplt.ylabel(\"Class Bool\")\nplt.grid(visible=True, which='major', axis='x')\nplt.title(\"The Change of Survival Probability by Lower Class (1 = 3rd Class)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Change of Survival Probability by Age\nfig = plt.figure(figsize=(18,9), dpi=1600)\na = .2\n\nfig.add_subplot(224, facecolor=\"#DBDBDB\")\nplt.scatter(res.predict(),x.Age , alpha=a)\nplt.grid(True, linewidth=0.15)\nplt.title(\"The Change of Survival Probability by Age\")\nplt.xlabel(\"Predicted chance of survival\")\nplt.ylabel(\"Age\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use our model to predict the test set values","metadata":{}},{"cell_type":"markdown","source":"#### Read the test data","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Examine our data frame","metadata":{}},{"cell_type":"code","source":"test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Add our independent variable to our test data. (It is usually left blank by Kaggle because it is the value you are trying to predict.)","metadata":{}},{"cell_type":"code","source":"test_data['Survived'] = 1.23","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Our binned results data:","metadata":{}},{"cell_type":"code","source":"results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use your model to make prediction on our test set (Only on Kaggle with kaggleaux)","metadata":{}},{"cell_type":"code","source":"# compared_results = ka.predict(test_data, results, 'Logit')\n# compared_results = Series(compared_resuts)  # convert our model to a series for easy output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Output and submit to kaggle (Only on Kaggle with kaggleaux)","metadata":{}},{"cell_type":"code","source":"# compared_resuts.to_csv(\"data/output/logitregres.csv\")\n# compared_resuts.to_csv(\"data/output/logitregres.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an acceptable formula for our machine learning algorithms\nformula_ml = 'Survived ~ C(Pclass) + C(Sex) + Age + SibSp + Parch + C(Embarked)'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine (SVM)","metadata":{}},{"cell_type":"code","source":"# Set plotting parameters\nplt.figure(figsize=(8,6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a regression friendly data frame\ny, x = dmatrices(formula_ml, data=df, return_type='matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select which features we would like to analyze\n# try chaning the selection here for diffrent output.\n# Choose : [2,3] - pretty sweet DBs [3,1] --standard DBs [7,3] -very cool DBs,\n# [3,6] -- very long complex dbs, could take over an hour to calculate! \nfeature_1 = 2\nfeature_2 = 3\n\nX = np.asarray(x)\nX = X[:,[feature_1, feature_2]]  \n\n\ny = np.asarray(y)\n# needs to be 1 dimenstional so we flatten. it comes out of dmatirces with a shape. \ny = y.flatten()      \n\nn_sample = len(X)\n\nnp.random.seed(0)\norder = np.random.permutation(n_sample)\n\nX = X[order]\ny = y[order].astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do a cross validation\nnighty_precent_of_sample = int(.9 * n_sample)\nX_train = X[:nighty_precent_of_sample]\ny_train = y[:nighty_precent_of_sample]\nX_test = X[nighty_precent_of_sample:]\ny_test = y[nighty_precent_of_sample:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a list of the types of kerneks we will use for your analysis\ntypes_of_kernels = ['linear', 'rbf', 'poly']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specify our color map for plotting the results\ncolor_map = plt.cm.RdBu_r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the model\nfor fig_num, kernel in enumerate(types_of_kernels):\n    clf = svm.SVC(kernel=kernel, gamma=3)\n    clf.fit(X_train, y_train)\n\n    plt.figure(fig_num)\n    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=color_map)\n\n    # circle out the test data\n    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)\n    \n    plt.axis('tight')\n    x_min = X[:, 0].min()\n    x_max = X[:, 0].max()\n    y_min = X[:, 1].min()\n    y_max = X[:, 1].max()\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.pcolormesh(XX, YY, Z > 0, cmap=color_map)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n               levels=[-.5, 0, .5])\n\n    plt.title(kernel)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Any value in the blue survived while anyone in the red did not. \n- Check out the graph for the linear transformation. It created its decision boundary right on 50%! That guess from earlier turned out to be pretty good. As you can see, the remaining decision boundaries are much more complex than our original linear decision boundary. These more complex boundaries may be able to capture more structure in the dataset if that structure exists, and so might create a more powerful predictive model.","metadata":{}},{"cell_type":"code","source":"# Here you can output which ever result you would like by changing the Kernel and clf.predict lines\n# Change kernel here to poly, rbf or linear\n# adjusting the gamma level also changes the degree to which the model is fitted\nclf = svm.SVC(kernel='poly', gamma=3).fit(X_train, y_train)                                                            \ny,x = dmatrices(formula_ml, data=test_data, return_type='dataframe')\n\n# Change the interger values within x.ix[:,[6,3]].dropna() explore the relationships between other \n# features. the ints are column postions. ie. [6,3] 6th column and the third column are evaluated. \nres_svm = clf.predict(x.iloc[:,[6,3]].dropna())\n\nres_svm = DataFrame(res_svm,columns=['Survived'])\nres_svm.to_csv(\"/kaggle/working//svm_poly.csv\") # saves the results for you, change the name as you please. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"# import the machine learning library that holds the randomforest\nimport sklearn.ensemble as ske\n\n# Create the random forest model and fit the model to our training data\ny, x = dmatrices(formula_ml, data=df, return_type='dataframe')\n# RandomForestClassifier expects a 1 demensional NumPy array, so we convert\ny = np.asarray(y).ravel()\n#instantiate and fit our model\nresults_rf = ske.RandomForestClassifier(n_estimators=100).fit(x, y)\n\n# Score the results\nscore = results_rf.score(x, y)\nprint (\"Mean accuracy of Random Forest Predictions on the data was: {0}\".format(score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}